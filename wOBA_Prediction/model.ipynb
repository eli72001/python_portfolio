{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"baseball-woba\"\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"bucket-name-placeholder\"\n",
    "bucket_uri = f\"gs://{bucket_name}\"\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "if bucket_name == \"\" or bucket_name is None or bucket_name == \"bucket-name-placeholder\":\n",
    "    bucket_name = PROJECT_ID + \"aip-\" + timestamp\n",
    "    bucket_uri = \"gs://\" + bucket_name\n",
    "\n",
    "bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage \n",
    "client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "bucket = client.create_bucket(bucket_name, location=LOCATION)\n",
    "\n",
    "print(\"Bucket {} created.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=bucket_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "LABEL_COLUMN = \"woba\" \n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df = df.drop(['last_name, first_name', 'player_id', 'year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.85, random_state=100)\n",
    "df_predict = df[~df.index.isin(df_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bq_dataset_id = f\"{PROJECT_ID}.dataset_id_unique\"\n",
    "bq_dataset = bigquery.Dataset(bq_dataset_id)\n",
    "bq_client.create_dataset(bq_dataset, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create_from_dataframe(\n",
    "    df_source=df_train,\n",
    "    staging_path=f\"bq://{bq_dataset_id}.table-unique\",\n",
    "    display_name=\"sample-baseball\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile task.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "# Read environmental variables\n",
    "training_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "# Read args\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--label_column', required=True, type=str)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--batch_size', default=10, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set up training variables\n",
    "LABEL_COLUMN = args.label_column\n",
    "\n",
    "# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.\n",
    "PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "bq_client = bigquery.Client(project=PROJECT_NUMBER)\n",
    "\n",
    "\n",
    "# Download a table\n",
    "def download_table(bq_table_uri: str):\n",
    "    # Remove bq:// prefix if present\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "        \n",
    "    # Download the BigQuery table as a dataframe\n",
    "    # This requires the \"BigQuery Read Session User\" role on the custom training service account.\n",
    "    table = bq_client.get_table(bq_table_uri)\n",
    "    return bq_client.list_rows(table).to_dataframe()\n",
    "\n",
    "# Download dataset splits\n",
    "df_train = download_table(training_data_uri)\n",
    "df_validation = download_table(validation_data_uri)\n",
    "df_test = download_table(test_data_uri)\n",
    "\n",
    "def convert_dataframe_to_dataset(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_validation: pd.DataFrame,\n",
    "):\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\n",
    "    y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\n",
    "    x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    num_species = len(df_train_y.unique())\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    return (dataset_train, dataset_validation)\n",
    "\n",
    "# Create datasets\n",
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)\n",
    "\n",
    "# Shuffle train set\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n",
    "\n",
    "def create_model(num_features):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                100,\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=\"uniform\",\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(75, activation=tf.nn.relu),\n",
    "            Dense(50, activation=tf.nn.relu),            \n",
    "            Dense(25, activation=tf.nn.relu),\n",
    "            Dense(3, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)\n",
    "\n",
    "# Set up datasets\n",
    "dataset_train = dataset_train.batch(args.batch_size)\n",
    "dataset_validation = dataset_validation.batch(args.batch_size)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)\n",
    "\n",
    "tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unique\"\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--label_column=\" + LABEL_COLUMN,\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\n",
    "    requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\", \"protobuf<3.20.0\"],\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"baseball_model_unique\"\n",
    "# Start the training and create your model\n",
    "model = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    "    args=CMDARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attempt didn't work. Trying a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client._credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "TRAINING_INPUT_DATASET_ID = \"baseball_training_unique\"\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{TRAINING_INPUT_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created Dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test dataset\n",
    "PREDICTION_INPUT_DATASET_ID = \"baseball_prediction_unique\"\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{PREDICTION_INPUT_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created Dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TabularDataset.create_from_dataframe(\n",
    "    df_source=df_train,\n",
    "    display_name=\"Baseball Train\",\n",
    "    staging_path=f\"bq://{PROJECT_ID}.{TRAINING_INPUT_DATASET_ID}.table-unique\"\n",
    "    #staging path\n",
    "    #df_source=df_train,\n",
    "    #staging_path=f\"bq://{bq_dataset_id}.table-unique\",\n",
    "    #display_name=\"sample-baseball\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job = aiplatform.AutoMLTabularTrainingJob(\n",
    "    display_name=\"job_unique2\",\n",
    "    optimization_prediction_type='regression',\n",
    "    optimization_objective='minimize-rmse',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"baseball-model\",\n",
    "    training_fraction_split=0.9,\n",
    "    validation_fraction_split=0.05,\n",
    "    test_fraction_split=0.05,\n",
    "    budget_milli_node_hours=1000,\n",
    "    disable_early_stopping=True,\n",
    "    target_column=LABEL_COLUMN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluations = model.list_model_evaluations()\n",
    "\n",
    "model_evaluations = list(model_evaluations)[0]\n",
    "print(model_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DATASET_ID = \"baseball_results_unique\"\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{RESULTS_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_RESULTS_DATASET_ID = f\"{PROJECT_ID}.{RESULTS_DATASET_ID}\"\n",
    "\n",
    "batch_job = model.batch_predict(\n",
    "    job_display_name=\"regression-prediction\",\n",
    "    bigquery_source=f\"bq://{PROJECT_ID}.{TRAINING_INPUT_DATASET_ID}.table-unique\",\n",
    "    instances_format=\"bigquery\",\n",
    "    predictions_format=\"bigquery\",\n",
    "    bigquery_destination_prefix=f\"bq://{PREDICTION_RESULTS_DATASET_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Select * from `{PREDICTION_RESULTS_DATASET_ID}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    bq_client.query(f\"SELECT predicted_woba.value, woba, ((predicted_woba.value-woba)/woba)*100 as err FROM `{PREDICTION_RESULTS_DATASET_ID}.*`\")\n",
    "    .result()\n",
    "    .to_dataframe()\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['err'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.delete()\n",
    "model.delete()\n",
    "training_job.delete()\n",
    "batch_job.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
