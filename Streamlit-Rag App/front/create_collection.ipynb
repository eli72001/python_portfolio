{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import chromadb\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredHTMLLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from openai import OpenAI\n",
    "from langchain_cohere import CohereRerank\n",
    "from langchain_community.llms import Cohere\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings, CohereRagRetriever, CohereRerank\n",
    "import chromadb.utils.embedding_functions as embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Loading, Cleaning, Chunking and Combine Chunks\n",
    "\n",
    "def clean_text(txt):\n",
    "    return txt.replace('\\n', ' ').replace('\\t', ' ') # Removes new line and tab characters\n",
    "\n",
    "def load_pdf(filename):\n",
    "    loader = PyPDFLoader(filename)\n",
    "    pattern = r'^[\\d/]+$'\n",
    "    clean_data = []\n",
    "    data = loader.load() # Returns a list of documents for every page of the pdf not cleaned\n",
    "    cik = get_cik(data[0])\n",
    "    cleaned = clean_pages(data)\n",
    "    for page in cleaned:\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "        if bool(re.match(pattern, page.page_content)) == False:\n",
    "            clean_data.append(page)\n",
    "    return cik, clean_data \n",
    "\n",
    "def clean_pages(pages): # Removes start of \n",
    "    for page in pages:\n",
    "        benchmark = -1\n",
    "        page_arr = page.page_content.split(' ')\n",
    "        for index, text in enumerate(page_arr):\n",
    "            if index > 10:\n",
    "                break\n",
    "            if 'https' in text:\n",
    "                benchmark = index\n",
    "                break\n",
    "        if benchmark > 0:\n",
    "            clean_content = page_arr[benchmark+1:]\n",
    "            page.page_content = ' '.join(clean_content)    \n",
    "\n",
    "    return pages\n",
    "\n",
    "def combine_chunks(docs, company, file_path, file_type, cik, n):\n",
    "    combine_docs = []\n",
    "    for i in range(len(docs)-n):\n",
    "        page_content = \"\"\n",
    "        pages_arr = []\n",
    "        for j in range(n+1):\n",
    "            page_content += docs[i+j].page_content\n",
    "            pages_arr.append(f\"{docs[i+j].metadata['page']}\")\n",
    "        pages_arr = [pages_arr[0], pages_arr[-1]]\n",
    "        if len(pages_arr) > 1:\n",
    "            pages = '-'.join(pages_arr)\n",
    "        else:\n",
    "            pages = pages_arr[0]\n",
    "        new_doc = Document(page_content=page_content, metadata={\"file path\": file_path, \"company\": company, \"file_type\": file_type, \"cik\": cik, \"page\": pages})\n",
    "        combine_docs.append(new_doc)\n",
    "    return combine_docs\n",
    "\n",
    "def chunk_combined_docs(combined_chunks, size):\n",
    "    final_chunks = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=size)\n",
    "    for doc in combined_chunks:\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "        for txt in chunks:\n",
    "            new_doc = Document(page_content=txt, metadata=doc.metadata)\n",
    "            final_chunks.append(new_doc)\n",
    "    return final_chunks\n",
    "\n",
    "def pretty_print(output_string):\n",
    "    for item in output_string.split('\\n'):\n",
    "        print(item)\n",
    "\n",
    "# Get the list of all files and directories\n",
    "def get_doc_names(path):\n",
    "    file_list = os.listdir(path)\n",
    "    return file_list\n",
    "\n",
    "# Get metadatas from file name\n",
    "def get_metadatas(file_name):\n",
    "    company = file_name.split()[0]\n",
    "    file_type = file_name.split()[1].split(\".\")[0]\n",
    "    return company, file_type\n",
    "\n",
    "# Add document chunks to collection\n",
    "def add_doc_to_collection(collection, final_chunks):\n",
    "    # Gather documents, metadatas and ids to add to collection\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    cur_count = collection.count()\n",
    "\n",
    "    for i in range(len(final_chunks)):\n",
    "        ids.append('id' + str(cur_count + i + 1))\n",
    "        documents.append(final_chunks[i].page_content)\n",
    "        metadatas.append(final_chunks[i].metadata)\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "\n",
    "def get_cik(page):\n",
    "    try:\n",
    "        for words in page.page_content.split(' '):\n",
    "            if 'https' in words:\n",
    "                splits = words.split('/')\n",
    "                nums = []\n",
    "                pattern = r'^\\d+$'\n",
    "                for s in splits:\n",
    "                    if re.match(pattern, s):\n",
    "                        nums.append(s)\n",
    "                if nums:\n",
    "                    return nums[0]\n",
    "                else:\n",
    "                    print(\"CIK could not be extracted\")\n",
    "                    return -1\n",
    "    except Exception as err:\n",
    "        print(\"CIK Could not be extracted from this document\")\n",
    "        return -1\n",
    "    print(\"CIK could not be extracted from this doc\")\n",
    "    return -1\n",
    "\n",
    "# Add json chunk to collection\n",
    "def add_json_to_collection(collection, json, file_path):\n",
    "    json_chunk = [str(json)]\n",
    "    cur_count = collection.count()\n",
    "    ids = ['id' + str(cur_count + 1)]\n",
    "\n",
    "    # Add to collection\n",
    "    collection.add(\n",
    "        documents=json_chunk,\n",
    "        metadatas=[{\"file path\": file_path, \"source\": \"OpenCorporates\", \"file_type\": \"json\"}],\n",
    "        ids=ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "# Specify db location\n",
    "db_path = \"db\"\n",
    "\n",
    "# Define collection name\n",
    "collection_name = \"docs_collection\"\n",
    "\n",
    "# Define embedding function\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.environ['OPENAI_API_KEY']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to chroma server\n",
    "client = chromadb.PersistentClient(path=db_path)\n",
    "\n",
    "# Uncomment to delete collection\n",
    "# client.delete_collection(name=\"docs_collection\")\n",
    "\n",
    "# Create a new collection\n",
    "collection = client.create_collection(name=collection_name, embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK could not be extracted from this doc\n"
     ]
    }
   ],
   "source": [
    "# Run once to create the base collection db\n",
    "file_list = get_doc_names(\"company-10ks\")\n",
    "\n",
    "for file in file_list:\n",
    "    # Get metadatas\n",
    "    company, file_type = get_metadatas(file)\n",
    "\n",
    "    # Load file\n",
    "    path = \"company-10ks/\" + file\n",
    "    cik, pdf_chunks = load_pdf(path)\n",
    "\n",
    "    # Preprocess file: cleaning, chunking, combining chunks\n",
    "    #clean_chunks = clean_pages(pdf_chunks)\n",
    "    combined_chunks = combine_chunks(pdf_chunks, company, file, file_type, cik, 1)\n",
    "    final_chunks = chunk_combined_docs(combined_chunks, 2000)\n",
    "\n",
    "    # Add final chunks with metadatas to chromadb collection\n",
    "    add_doc_to_collection(collection, final_chunks)\n",
    "\n",
    "json_list = get_doc_names(\"company_json\")\n",
    "\n",
    "for json_file in json_list:\n",
    "    path = \"company_json/\" + json_file\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    add_json_to_collection(collection, data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection.peek()\n",
    "# collection.peek().keys()\n",
    "# collection.peek()[\"documents\"]\n",
    "collection.peek()[\"metadatas\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
